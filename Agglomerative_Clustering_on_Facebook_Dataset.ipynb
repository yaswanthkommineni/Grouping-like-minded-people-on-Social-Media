{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwAdV_2g6cMe"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrO7n82SI4kb",
        "outputId": "d142cfb8-07bd-4ee8-80d4-d2e56566bfe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Uploading dataset to the google drive and mounting drive to access the csv files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3hpJrqrKXGJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/Facebook-dataset/dataset/'\n",
        "\n",
        "comments = pd.read_csv(dataset_path + 'comment.csv')\n",
        "likes = pd.read_csv(dataset_path + 'like.csv')\n",
        "posts = pd.read_csv(dataset_path + 'post.csv')\n",
        "members = pd.read_csv(dataset_path + 'member.csv')\n",
        "\n",
        "# converting all id's to string to avoid further complexities\n",
        "\n",
        "comments[\"msg\"].values.astype(str)\n",
        "\n",
        "for column in comments:\n",
        "  if(column[-2:] == 'id'):\n",
        "    comments[column] = comments[column].values.astype(str)\n",
        "\n",
        "for column in likes:\n",
        "  if(column[-2:] == 'id'):\n",
        "    likes[column] = likes[column].values.astype(str)\n",
        "\n",
        "posts[\"msg\"].values.astype(str)\n",
        "\n",
        "for column in posts:\n",
        "  if(column[-2:] == 'id'):\n",
        "    posts[column] = posts[column].values.astype(str)\n",
        "\n",
        "for column in members:\n",
        "  if(column[-2:] == 'id'):\n",
        "    members[column] = members[column].values.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxep0LVwOsiB"
      },
      "outputs": [],
      "source": [
        "# comment_like_count contains the number of likes a comment has\n",
        "comment_like_count = dict()\n",
        "\n",
        "for i in range(len(likes)):\n",
        "  cid = likes.iloc[i]['cid']\n",
        "  if cid == 'x':\n",
        "    # we only want likes for the comments\n",
        "    continue\n",
        "  if cid in comment_like_count.keys():\n",
        "    comment_like_count[cid]+=1\n",
        "  else:\n",
        "    comment_like_count[cid] = 1\n",
        "\n",
        "# comment_post_like contains comment text combined with number of likes that are associated to each post\n",
        "import math\n",
        "\n",
        "comment_post_like = dict()\n",
        "\n",
        "for i in range(len(comments)):\n",
        "  pid = comments.iloc[i]['pid']\n",
        "  cid = comments.iloc[i]['cid']\n",
        "  text = comments.iloc[i]['msg']\n",
        "  # we don't need the comments of comments\n",
        "  if comments.iloc[i]['rid'] != \"nan\":\n",
        "    continue\n",
        "  if cid in comment_like_count.keys():\n",
        "    like_count = comment_like_count[cid]\n",
        "  if pid in comment_post_like.keys():\n",
        "    comment_post_like[pid].append((cid,text,like_count))\n",
        "  else:\n",
        "    comment_post_like[pid] = [(cid,text,like_count)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmjPLXd66U3v"
      },
      "source": [
        "# Keyword Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6ph1gTxSg1M"
      },
      "outputs": [],
      "source": [
        "# storing the text for each post in the dictionary\n",
        "\n",
        "post_text = dict()\n",
        "post_likes = dict()\n",
        "import numpy\n",
        "\n",
        "for i in range(len(posts)):\n",
        "  post_text[posts.iloc[i][\"pid\"]] = posts.iloc[i][\"msg\"]\n",
        "  val = posts.iloc[i][\"likes\"]\n",
        "  if math.isnan(val):\n",
        "    val = numpy.int64(0)\n",
        "  post_likes[posts.iloc[i][\"pid\"]] = val\n",
        "\n",
        "# sorting comments based on the likes they've got\n",
        "\n",
        "for pid in comment_post_like.keys():\n",
        "  comment_post_like[pid] = sorted(comment_post_like[pid], key = lambda x : x[2], reverse = True)\n",
        "\n",
        "# to store the text that represents post (post text + comment text)\n",
        "\n",
        "post_representation = dict()\n",
        "\n",
        "# adding post text to the post_representation\n",
        "\n",
        "for (i,x) in post_text.items():\n",
        "  post_representation[i] = post_text[i]\n",
        "  if(type(post_representation[i]) == type(8.9)):\n",
        "    post_representation[i] = ''\n",
        "\n",
        "# adding comment text to the post_representation\n",
        "\n",
        "for pid in comment_post_like.keys():\n",
        "  if pid not in post_text.keys():\n",
        "    continue\n",
        "  for (cid, message, numlikes) in comment_post_like[pid]:\n",
        "    # empty messages are stored as float in the dataframes\n",
        "    if(type(message) == type(8.9)):\n",
        "      continue\n",
        "    # filtering only the main comments to be used to generate the keywords\n",
        "    if numlikes >= post_likes[pid]/5 and numlikes > 0:\n",
        "      post_representation[pid] += message + ' '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddQhw4qCXeGq"
      },
      "outputs": [],
      "source": [
        "# for regular expressions\n",
        "\n",
        "import re\n",
        "\n",
        "def prepreprocess(x):\n",
        "  res = ''\n",
        "  bracket_open = False\n",
        "  btw_text = ''\n",
        "  for c in x:\n",
        "    if c == '{':\n",
        "      bracket_open = True\n",
        "    elif c == '}' and bracket_open:\n",
        "      if btw_text == 'COMMA':\n",
        "        res += \",\"\n",
        "      elif btw_text == \"APOST\":\n",
        "        res += \"'\"\n",
        "      btw_text = ''\n",
        "      bracket_open = False\n",
        "    elif not bracket_open:\n",
        "      res += c\n",
        "    else:\n",
        "      btw_text += c\n",
        "\n",
        "  return re.sub(r'http\\S+', '', res)\n",
        "\n",
        "# pre-preprocessing\n",
        "# removing {} and links from the text of post_representation\n",
        "\n",
        "for (i,x) in post_representation.items():\n",
        "  post_representation[i] = prepreprocess(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjN_1sepBRr8"
      },
      "source": [
        "## TextRank algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyb-aYwmtOHv",
        "outputId": "236f3cf3-d03c-43bc-8f2a-60d1eaec4423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# importing necessary libraries\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08NS2obxhVns"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "  text = text.lower()\n",
        "  printable = set(string.printable)\n",
        "  text = list(filter(lambda x: x in printable, text)) #filter funny characters, if any.\n",
        "  res = ''\n",
        "  for x in text:\n",
        "    res += x\n",
        "  return res\n",
        "\n",
        "def extract_keywords(Text):\n",
        "  Cleaned_text = clean(Text)\n",
        "\n",
        "  res = ''\n",
        "  \n",
        "  for x in Cleaned_text:\n",
        "    if (ord(x) <= ord('z') and ord(x) >= ord('a')) or (ord(x) <= ord('Z') and ord(x) >= ord('A')) or x == \"'\":\n",
        "      res += x\n",
        "    elif x == '.':\n",
        "      res += x\n",
        "      res += ' '\n",
        "    else:\n",
        "      res += ' '\n",
        "    \n",
        "  Cleaned_text = res\n",
        "\n",
        "  text = word_tokenize(Cleaned_text)\n",
        "\n",
        "  # POS tagging\n",
        "\n",
        "  POS_tag = nltk.pos_tag(text)\n",
        "\n",
        "  # Lemmatization\n",
        "\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "  adjective_tags = ['JJ','JJR','JJS']\n",
        "  lemmatized_text = []\n",
        "  for word in POS_tag:\n",
        "    if word[1] in adjective_tags:\n",
        "      lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
        "    else:\n",
        "      lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
        "  \n",
        "  POS_tag = nltk.pos_tag(lemmatized_text)\n",
        "\n",
        "  # POS based filtering\n",
        "\n",
        "  stopwords = []\n",
        "\n",
        "  wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
        "\n",
        "  for word in POS_tag:\n",
        "    if word[1] not in wanted_POS:\n",
        "      stopwords.append(word[0])\n",
        "  \n",
        "  punctuations = list(str(string.punctuation))\n",
        "\n",
        "  stopwords = stopwords + punctuations\n",
        "\n",
        "  # complete stopword generation\n",
        "\n",
        "  stopword_file = open(dataset_path + \"long_stopwords.txt\",\"r\")\n",
        "\n",
        "  lots_of_stopwords = []\n",
        "\n",
        "  for line in stopword_file.readlines():\n",
        "    lots_of_stopwords.append(str(line.strip()))\n",
        "  \n",
        "  stopwords_plus = []\n",
        "  stopwords_plus = stopwords + lots_of_stopwords\n",
        "  stopwords_plus = set(stopwords_plus)\n",
        "\n",
        "  # removing stopwords\n",
        "\n",
        "  processed_text = []\n",
        "  for word in lemmatized_text:\n",
        "      if word not in stopwords_plus:\n",
        "          processed_text.append(word)\n",
        "\n",
        "  # vocabulary creation\n",
        "\n",
        "  vocabulary = list(set(processed_text))\n",
        "\n",
        "  # graph creation\n",
        "\n",
        "  vocab_len = len(vocabulary)\n",
        "\n",
        "  weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
        "\n",
        "  score = np.zeros((vocab_len),dtype=np.float32)\n",
        "  window_size = 3\n",
        "  covered_coocurrences = []\n",
        "\n",
        "  for i in range(0,vocab_len):\n",
        "      score[i]=1\n",
        "      for j in range(0,vocab_len):\n",
        "          if j==i:\n",
        "              weighted_edge[i][j]=0\n",
        "          else:\n",
        "              for window_start in range(0,(len(processed_text)-window_size+1)):\n",
        "                  \n",
        "                  window_end = window_start+window_size\n",
        "                  \n",
        "                  window = processed_text[window_start:window_end]\n",
        "                  \n",
        "                  if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
        "                      \n",
        "                      index_of_i = window_start + window.index(vocabulary[i])\n",
        "                      index_of_j = window_start + window.index(vocabulary[j])\n",
        "                      \n",
        "                      # index_of_x is the absolute position of the xth term in the window \n",
        "                      # (counting from 0) \n",
        "                      # in the processed_text\n",
        "                        \n",
        "                      if [index_of_i,index_of_j] not in covered_coocurrences:\n",
        "                          weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
        "                          covered_coocurrences.append([index_of_i,index_of_j])\n",
        "\n",
        "  # calculation weighted summation of connections of a vertex\n",
        "\n",
        "  inout = np.zeros((vocab_len),dtype=np.float32)\n",
        "\n",
        "  for i in range(0,vocab_len):\n",
        "      for j in range(0,vocab_len):\n",
        "          inout[i]+=weighted_edge[i][j]\n",
        "  \n",
        "  # scoring vertices\n",
        "\n",
        "  MAX_ITERATIONS = 10\n",
        "  d=0.85\n",
        "  threshold = 0.0001 #convergence threshold\n",
        "\n",
        "  for iter in range(0,MAX_ITERATIONS):\n",
        "    prev_score = np.copy(score)\n",
        "    \n",
        "    for i in range(0,vocab_len):\n",
        "      \n",
        "      summation = 0\n",
        "      for j in range(0,vocab_len):\n",
        "        if weighted_edge[i][j] != 0:\n",
        "          summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
        "              \n",
        "      score[i] = (1-d) + d*(summation)\n",
        "    \n",
        "    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
        "      break\n",
        "        \n",
        "  # phrase partitioning\n",
        "\n",
        "  phrases = []\n",
        "\n",
        "  phrase = \" \"\n",
        "  for word in lemmatized_text:\n",
        "    \n",
        "    if word in stopwords_plus:\n",
        "      if phrase!= \" \":\n",
        "          phrases.append(str(phrase).strip().split())\n",
        "      phrase = \" \"\n",
        "    elif word not in stopwords_plus:\n",
        "      phrase+=str(word)\n",
        "      phrase+=\" \"\n",
        "    \n",
        "  phrases = []\n",
        "\n",
        "  phrase = \" \"\n",
        "  for word in lemmatized_text:\n",
        "    \n",
        "    if word in stopwords_plus:\n",
        "      if phrase!= \" \":\n",
        "        phrases.append(str(phrase).strip().split())\n",
        "      phrase = \" \"\n",
        "    elif word not in stopwords_plus:\n",
        "      phrase+=str(word)\n",
        "      phrase+=\" \"\n",
        "  \n",
        "  # creating a list of unique phrases\n",
        "\n",
        "  unique_phrases = []\n",
        "\n",
        "  for phrase in phrases:\n",
        "    if phrase not in unique_phrases:\n",
        "      unique_phrases.append(phrase)\n",
        "  \n",
        "\n",
        "  # thinning the list of candidate-keyphrases\n",
        "\n",
        "  for word in vocabulary:\n",
        "    #print word\n",
        "    for phrase in unique_phrases:\n",
        "      if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
        "        #if len(phrase)>1 then the current phrase is multi-worded.\n",
        "        #if the word in vocabulary is present in unique_phrases as a single-word-phrase\n",
        "        # and at the same time present as a word within a multi-worded phrase,\n",
        "        # then I will remove the single-word-phrase from the list.\n",
        "        unique_phrases.remove([word])\n",
        "  \n",
        "  # scoring keyphrases\n",
        "\n",
        "  phrase_scores = []\n",
        "  keywords = []\n",
        "  for phrase in unique_phrases:\n",
        "    phrase_score=0\n",
        "    keyword = ''\n",
        "    for word in phrase:\n",
        "        keyword += str(word)\n",
        "        keyword += \" \"\n",
        "        phrase_score+=score[vocabulary.index(word)]\n",
        "    phrase_scores.append(phrase_score)\n",
        "    keywords.append(keyword.strip())\n",
        "\n",
        "  i=0\n",
        "\n",
        "  key_scores = dict()\n",
        "\n",
        "  for keyword in keywords:\n",
        "    key_scores[keyword] = phrase_scores[i]\n",
        "    i+=1\n",
        "\n",
        "  return key_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkIVOdAG8V5j",
        "outputId": "642851f8-ea70-4d81-9e01-47e313ce2086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "posts completed : 1000\n",
            "posts completed : 2000\n",
            "posts completed : 3000\n",
            "posts completed : 4000\n",
            "posts completed : 5000\n",
            "posts completed : 6000\n",
            "posts completed : 7000\n",
            "posts completed : 8000\n",
            "posts completed : 9000\n",
            "posts completed : 10000\n",
            "posts completed : 11000\n",
            "posts completed : 12000\n",
            "posts completed : 13000\n",
            "posts completed : 14000\n",
            "posts completed : 15000\n",
            "posts completed : 16000\n",
            "posts completed : 17000\n",
            "posts completed : 18000\n",
            "posts completed : 19000\n",
            "posts completed : 20000\n"
          ]
        }
      ],
      "source": [
        "j = 0\n",
        "\n",
        "keyword_scores = dict()\n",
        "\n",
        "for (i,x) in post_representation.items():\n",
        "  keyword_scores[i] = extract_keywords(x)\n",
        "  j += 1\n",
        "  if(j%1000 == 0):\n",
        "    print(\"posts completed :\",j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWj9Ug6311Kg"
      },
      "outputs": [],
      "source": [
        "# for counting the number of words in the given string\n",
        "\n",
        "def count_words(text):\n",
        "  result = 0\n",
        "  cur = \"\"\n",
        "  for x in text:\n",
        "    if x == ' ' and len(cur) > 0:\n",
        "      result+=1\n",
        "      cur = \"\"\n",
        "      continue\n",
        "    cur += x\n",
        "  return result+1\n",
        "\n",
        "# for getting individual words from the given string/sentances\n",
        "\n",
        "def get_words(text):\n",
        "  cur = \"\"\n",
        "  res = []\n",
        "  for x in text:\n",
        "    if x == ' ':\n",
        "      if len(cur) > 0:\n",
        "        res.append(cur)\n",
        "      cur = \"\"\n",
        "    else:\n",
        "      cur += x\n",
        "  if len(cur) > 0:\n",
        "    res.append(cur)\n",
        "  return res\n",
        "\n",
        "# for \n",
        "\n",
        "def combine(scores):\n",
        "  for (id, sc) in scores.items():\n",
        "    cur = dict()\n",
        "    for (word, score) in sc.items():\n",
        "      if count_words(word) > 1:\n",
        "        for x in get_words(word):\n",
        "          if x in cur.keys():\n",
        "            cur[x] += score/2\n",
        "          else:\n",
        "            cur[x] = score\n",
        "      else:\n",
        "        if word in cur.keys():\n",
        "          cur[word] += score\n",
        "        else:\n",
        "          cur[word] = score\n",
        "    scores[id] = cur\n",
        "  return scores\n",
        "\n",
        "textrankscores = combine(keyword_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr7eXBQBjoB5"
      },
      "source": [
        "## TF-IDF ranking based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc04OavbjwBJ"
      },
      "outputs": [],
      "source": [
        "# In tf-idf, we are also considering the replies for the first comments (recomments)\n",
        "\n",
        "post_recomments = dict()\n",
        "\n",
        "for i in range(len(comments)):\n",
        "  pid = comments.iloc[i]['pid']\n",
        "  cid = comments.iloc[i]['cid']\n",
        "  text = comments.iloc[i]['msg']\n",
        "  # we only need the comments of comments\n",
        "  if comments.iloc[i]['rid'] == \"nan\":\n",
        "    continue\n",
        "  if(type(text) == type(9.8)):\n",
        "    text = ''\n",
        "  if pid in post_recomments.keys():\n",
        "    post_recomments[pid] += text\n",
        "  else:\n",
        "    post_recomments[pid] = text\n",
        "\n",
        "# considering the entire text (post text, comments and replies of comments) as a single document\n",
        "\n",
        "idf_post_representation = dict()\n",
        "\n",
        "for (i,x) in post_text.items():\n",
        "  if type(x) == type(8.9):\n",
        "    idf_post_representation[i] = ''\n",
        "  else:\n",
        "    idf_post_representation[i] = x + ' '\n",
        "\n",
        "\n",
        "# adding comment text to the post_representation\n",
        "\n",
        "for pid in comment_post_like.keys():\n",
        "  text = ''\n",
        "  for (cid, message, numlikes) in comment_post_like[pid]:\n",
        "    if(type(message) == type(8.9)):\n",
        "      continue\n",
        "    text += message\n",
        "    text += ' '\n",
        "  if pid not in idf_post_representation.keys():\n",
        "    idf_post_representation[pid] = text\n",
        "  else:\n",
        "    idf_post_representation[pid] += text\n",
        "\n",
        "for (i,x) in post_recomments.items():\n",
        "  if i not in idf_post_representation.keys():\n",
        "    idf_post_representation[i] = x + ' '\n",
        "  else:\n",
        "    idf_post_representation[i] += x + ' '\n",
        "\n",
        "for (i,x) in idf_post_representation.items():\n",
        "  idf_post_representation[i] = prepreprocess(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUZf1zOW4f2x"
      },
      "outputs": [],
      "source": [
        "# performing the necessary operations for preprocessing the documents (tweet texts) to apply the TF-IDF ranking algorithm\n",
        "\n",
        "def process_docs(doclist):\n",
        "  docs = []\n",
        "  for Text in doclist:\n",
        "    Cleaned_text = clean(Text)\n",
        "    res = ''\n",
        "    \n",
        "    for x in Cleaned_text:\n",
        "      if (ord(x) <= ord('z') and ord(x) >= ord('a')) or (ord(x) <= ord('Z') and ord(x) >= ord('A')) or x == \"'\":\n",
        "        res += x\n",
        "      elif x == '.':\n",
        "        res += x\n",
        "        res += ' '\n",
        "      else:\n",
        "        res += ' '\n",
        "      \n",
        "    Cleaned_text = res\n",
        "\n",
        "    text = word_tokenize(Cleaned_text)\n",
        "\n",
        "    # POS tagging\n",
        "\n",
        "    POS_tag = nltk.pos_tag(text)\n",
        "\n",
        "    # Lemmatization\n",
        "\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    adjective_tags = ['JJ','JJR','JJS']\n",
        "    lemmatized_text = []\n",
        "    for word in POS_tag:\n",
        "      if word[1] in adjective_tags:\n",
        "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
        "      else:\n",
        "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
        "    \n",
        "    POS_tag = nltk.pos_tag(lemmatized_text)\n",
        "\n",
        "    # POS based filtering\n",
        "\n",
        "    stopwords = []\n",
        "\n",
        "    wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
        "\n",
        "    for word in POS_tag:\n",
        "      if word[1] not in wanted_POS:\n",
        "        stopwords.append(word[0])\n",
        "  \n",
        "    punctuations = list(str(string.punctuation))\n",
        "\n",
        "    stopwords = stopwords + punctuations\n",
        "\n",
        "    # complete stopword generation\n",
        "\n",
        "    stopword_file = open(dataset_path + \"long_stopwords.txt\",\"r\")\n",
        "\n",
        "    lots_of_stopwords = []\n",
        "\n",
        "    for line in stopword_file.readlines():\n",
        "      lots_of_stopwords.append(str(line.strip()))\n",
        "    \n",
        "    stopwords_plus = []\n",
        "    stopwords_plus = lots_of_stopwords\n",
        "    stopwords_plus = set(stopwords_plus)\n",
        "\n",
        "    # removing stopwords\n",
        "\n",
        "    processed_text = []\n",
        "    for word in lemmatized_text:\n",
        "      if word not in lots_of_stopwords:\n",
        "        processed_text.append(word)\n",
        "\n",
        "    res = \"\"\n",
        "    for x in processed_text:\n",
        "      if len(res) > 0:\n",
        "        res += \" \"\n",
        "      res += x\n",
        "    docs.append(res)\n",
        "  \n",
        "  return docs\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qEObMO2IZtq"
      },
      "outputs": [],
      "source": [
        "def pre_process(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(\"&lt;/?.*&gt;\",\"&lt;&gt; \", text)\n",
        "  text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "  return text\n",
        "\n",
        "for (i,x) in idf_post_representation.items():\n",
        "  idf_post_representation[i] = pre_process(x)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def get_stop_words(stop_file_path):\n",
        "  with open(stop_file_path,'r', encoding = 'utf-8') as f:\n",
        "    stopwords = f.readlines()\n",
        "    stop_set = set(m.strip() for m in stopwords)\n",
        "    return stop_set\n",
        "\n",
        "stopwords = get_stop_words(dataset_path + \"stopwords.txt\")\n",
        "\n",
        "docs = []\n",
        "\n",
        "for (i,x) in idf_post_representation.items():\n",
        "  docs.append(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = process_docs(docs)"
      ],
      "metadata": {
        "id": "OGXdCu3Uqxlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.feature_extraction import text \n",
        "# stop_words = text.ENGLISH_STOP_WORDS.union(stopwords)"
      ],
      "metadata": {
        "id": "PPpn3Ny95HiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(list(stopwords)))\n",
        "# print(type(docs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyM0kKr8zSWg",
        "outputId": "833c2cb3-dc58-4db7-c6eb-afb3607c58b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(max_df=0.85,stop_words = list(stopwords))\n",
        "word_count_vector = cv.fit_transform(docs)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
        "tfidf_transformer.fit(word_count_vector)\n",
        "\n",
        "feature_names = cv.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIgJPW2Bqyjh",
        "outputId": "029b749d-768e-45c1-c754-3b3f10ed2288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6u5Q9xxbpsi"
      },
      "outputs": [],
      "source": [
        "def extract(coo_matrix):\n",
        "  tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "  score_vals = []\n",
        "  feature_vals = []\n",
        "\n",
        "  for idx, score in sorted(tuples):\n",
        "    score_vals.append(score)\n",
        "    feature_vals.append(feature_names[idx])\n",
        "  \n",
        "  result = dict()\n",
        "\n",
        "  for idx in range(len(feature_vals)):\n",
        "    result[feature_vals[idx]] = score_vals[idx]\n",
        "  \n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH-JqLAnHpAq"
      },
      "outputs": [],
      "source": [
        "def map_scale1_convert(scores):\n",
        "  mx = 0\n",
        "  for (word, score) in scores.items():\n",
        "    mx = max(mx, score)\n",
        "  for (word, score) in scores.items():\n",
        "    scores[word] = score/mx\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icTBJt2WWusx",
        "outputId": "8b9af0dd-6263-4ecb-bda8-32e6175daea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "posts completed : 1000\n",
            "posts completed : 2000\n",
            "posts completed : 3000\n",
            "posts completed : 4000\n",
            "posts completed : 5000\n",
            "posts completed : 6000\n",
            "posts completed : 7000\n",
            "posts completed : 8000\n",
            "posts completed : 9000\n",
            "posts completed : 10000\n",
            "posts completed : 11000\n",
            "posts completed : 12000\n",
            "posts completed : 13000\n",
            "posts completed : 14000\n",
            "posts completed : 15000\n",
            "posts completed : 16000\n",
            "posts completed : 17000\n",
            "posts completed : 18000\n",
            "posts completed : 19000\n",
            "posts completed : 20000\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "keywords = dict()\n",
        "\n",
        "postcount = 0\n",
        "\n",
        "Count = 0\n",
        "\n",
        "for pid in post_text.keys():\n",
        "  postcount += 1\n",
        "  if postcount % 1000 == 0:\n",
        "    print('posts completed :', postcount)\n",
        "  text = post_text[pid]\n",
        "  if type(text) == type(8.9):\n",
        "    text = ''\n",
        "  text = prepreprocess(text)\n",
        "  text = process_docs([text])\n",
        "  text = text[0]\n",
        "  post_scores = extract(tfidf_transformer.transform(cv.transform([text])).tocoo())\n",
        "\n",
        "  if pid in comment_post_like.keys():\n",
        "    num_comments = 0\n",
        "    total_likes = 0\n",
        "    for (cid, message, numlikes) in comment_post_like[pid]:\n",
        "      num_comments += 1\n",
        "      total_likes += numlikes\n",
        "\n",
        "    multiplier = dict()\n",
        "    ctext = ''\n",
        "    for (cid, message, numlikes) in comment_post_like[pid]:\n",
        "      if(type(message) == type(8.9)):\n",
        "        continue\n",
        "      ctext += prepreprocess(message) + ' '\n",
        "\n",
        "      comment_scores = extract(tfidf_transformer.transform(cv.transform([prepreprocess(message)])).tocoo())\n",
        "\n",
        "      for (word, score) in comment_scores.items():\n",
        "        if word in multiplier.keys():\n",
        "          multiplier[word] += (1 + numlikes)\n",
        "        else:\n",
        "          multiplier[word] = (1 + numlikes)\n",
        "\n",
        "  comment_scores = extract(tfidf_transformer.transform(cv.transform([ctext])).tocoo())\n",
        "  for (word, score) in comment_scores.items():\n",
        "    if word in post_scores.keys():\n",
        "      post_scores[word] += max(multiplier[word]/(num_comments+total_likes), 1/2)*score\n",
        "    else:\n",
        "      post_scores[word] = max(multiplier[word]/(num_comments+total_likes), 1/3)*score\n",
        "\n",
        "  recomment_scores = dict()\n",
        "  if pid in recomment_scores.keys():\n",
        "    recomment_scores = extract(tfidf_transformer.transform(cv.transform([prepreprocess(post_recomments[pid])])).tocoo())\n",
        "\n",
        "  for (word, score) in recomment_scores.items():\n",
        "    if word in post_scores.keys():\n",
        "      post_scores[word] += 1/3*score\n",
        "    else:\n",
        "      post_scores[word] = 1/10*score\n",
        "  \n",
        "  post_scores = map_scale1_convert(post_scores)\n",
        "\n",
        "  textrankscores[pid] = map_scale1_convert(textrankscores[pid])\n",
        "\n",
        "  for (word, score) in textrankscores[pid].items():\n",
        "    if word in post_scores.keys():\n",
        "      post_scores[word] += score\n",
        "    else:\n",
        "      post_scores[word] = score\n",
        "\n",
        "  post_scores = dict(sorted(post_scores.items(), key=lambda item: item[1], reverse = True))\n",
        "\n",
        "  num_keywords = max(5, count_words(text)/4)\n",
        "  num_keywords = min(25, num_keywords)\n",
        "\n",
        "  words = []\n",
        "\n",
        "  count = 0\n",
        "  for (word, scores) in post_scores.items():\n",
        "    if(count >= num_keywords):\n",
        "      break\n",
        "    count += 1\n",
        "    words.append(word)\n",
        "      \n",
        "  keywords[pid] = words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tag network and kernel diffusion"
      ],
      "metadata": {
        "id": "27Ip8nfS4FPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TJsbRSKq_6y"
      },
      "outputs": [],
      "source": [
        "def remove_singles(keywords):\n",
        "  word_count = dict()\n",
        "  for (id, kwords) in keywords.items():\n",
        "    for x in kwords:\n",
        "      if x in word_count.keys():\n",
        "        word_count[x] += 1\n",
        "      else:\n",
        "        word_count[x] = 1\n",
        "  res = dict()\n",
        "  for (id, kwords) in keywords.items():\n",
        "    newwords = []\n",
        "    for x in kwords:\n",
        "      if word_count[x] > 1:\n",
        "        newwords.append(x)\n",
        "    res[id] = newwords\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBCN2boyrp_m"
      },
      "outputs": [],
      "source": [
        "# removing multiple worded keywords and combining them into one\n",
        "\n",
        "keywords = remove_singles(keywords)\n",
        "\n",
        "# maintaining number to represent the tags instead of the strings\n",
        "tags = []\n",
        "tagHash = dict()\n",
        "\n",
        "for (id, kwords) in keywords.items():\n",
        "  for x in kwords:\n",
        "    if x in tagHash.keys():\n",
        "      continue\n",
        "    tagHash[x] = len(tags)\n",
        "    tags.append(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL_XUf-JLOw3"
      },
      "outputs": [],
      "source": [
        "# userposts is a dictionary mapping userids to the list of posts of the user\n",
        "\n",
        "userposts = dict()\n",
        "\n",
        "# iterating through the posts DataFrame and populating the userposts datastructure\n",
        "\n",
        "for i in range(len(posts)):\n",
        "  userid = posts.iloc[i]['id']\n",
        "  if userid in userposts.keys():\n",
        "    userposts[userid].append(posts.iloc[i]['pid'])\n",
        "  else:\n",
        "    userposts[userid] = [posts.iloc[i]['pid']]\n",
        "\n",
        "# it's easier to maintain strings in tags in form of numbers than the strings themselves\n",
        "# so replacing the tags in the keywords datastructure with their respective hash values\n",
        "\n",
        "for (id, kwords) in keywords.items():\n",
        "  hashed = []\n",
        "  for x in kwords:\n",
        "    hashed.append(tagHash[x])\n",
        "  keywords[id] = hashed\n",
        "\n",
        "num_tags = len(tags)\n",
        "\n",
        "# creating tag network \n",
        "\n",
        "tag_network = np.zeros([num_tags, num_tags])\n",
        "\n",
        "# iterating through all the posts posted by the user and populating the tag_network giving the appropriate weights\n",
        "# note: the weight of tag_network[x][y] is number of users who have used both the tags x and y simultaneously\n",
        "\n",
        "for (id, posts) in userposts.items():\n",
        "  # howmany times a user might use the same pair of keywords simultaneously, we only consider it once\n",
        "  paircount = dict()\n",
        "  for pid in posts:\n",
        "    # for every post iterating through all the pairwise tags\n",
        "    if pid not in keywords.keys():\n",
        "      continue\n",
        "    for i in range(len(keywords[pid])):\n",
        "      for j in range(i+1):\n",
        "        x = keywords[pid][i]\n",
        "        y = keywords[pid][j]\n",
        "        if x > y:\n",
        "          x,y = y,x\n",
        "        if (x,y) in paircount.keys():\n",
        "          paircount[(x,y)] += 1\n",
        "        else:\n",
        "          paircount[(x,y)] = 1\n",
        "  for (x,y) in paircount.keys():\n",
        "    tag_network[x][y] += 1\n",
        "    tag_network[y][x] += 1\n",
        "\n",
        "# representing a user with number between 0 - num_users rather than the long user_id  \n",
        "userHash = dict()\n",
        "# contains all the userids\n",
        "userids = []\n",
        "\n",
        "for id in userposts.keys():\n",
        "  if id in userHash.keys():\n",
        "    continue\n",
        "  userHash[id] = len(userids)\n",
        "  userids.append(id)\n",
        "\n",
        "num_users = len(userids)\n",
        "\n",
        "\n",
        "newuserposts = dict()\n",
        "\n",
        "for (id, posts) in userposts.items():\n",
        "  newuserposts[userHash[id]] = posts\n",
        "\n",
        "userposts = newuserposts\n",
        "\n",
        "usertagweights = np.zeros([num_users, num_tags])\n",
        "\n",
        "for (id, posts) in userposts.items():\n",
        "  for pid in posts:\n",
        "    if pid not in keywords.keys():\n",
        "      continue\n",
        "    for tag in keywords[pid]:\n",
        "      usertagweights[id][tag] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnM98288tIeX"
      },
      "outputs": [],
      "source": [
        "# Has some functionalities that are required to calculate the kernel\n",
        "import scipy.linalg\n",
        "\n",
        "\n",
        "# this class is to find the exponential of the given matrix (also known as kernel exponentiation)\n",
        "# note: only works for symmetric matrices\n",
        "\n",
        "class kernelexponentiation:\n",
        "  def __init__(self, mat):\n",
        "\n",
        "    self.n = len(mat)\n",
        "\n",
        "    # column sum is an array storing the sum of elements in each column\n",
        "\n",
        "    column_sum = numpy.zeros(self.n)\n",
        "    for i in range(self.n):\n",
        "      for j in range(self.n):\n",
        "        column_sum[j] += mat[i][j]\n",
        "\n",
        "    D = numpy.diag(column_sum)\n",
        "\n",
        "    # finding the negated leplacian matrix which helps in calculatint the exponential of a matrix\n",
        "\n",
        "    self.mat = numpy.subtract(mat, D)\n",
        "\n",
        "    # self.EV contains the eigen values of calculated negated leplacians matrix\n",
        "    # similarly self.V contains the eigen vectors and self.VT is the transpose of eigen vectors \n",
        "    curres = numpy.linalg.eig(self.mat)\n",
        "    self.EV = curres[0]\n",
        "    self.V = curres[1]\n",
        "    self.VT = numpy.transpose(self.V)\n",
        "\n",
        "  # computing the exponential using the formuala\n",
        "  def get_kernel(self,beta):\n",
        "    res = numpy.matmul(self.V, scipy.linalg.expm(numpy.diag(beta*self.EV)))\n",
        "    res = numpy.matmul(res, self.VT)\n",
        "    for i in range(num_tags):\n",
        "      for j in range(num_tags):\n",
        "        # removing the complex values\n",
        "        res[i][j] = res[i][j].real\n",
        "    return res\n",
        "\n",
        "# takes range [l,r] and returns n unique numbers from the range randomly\n",
        "\n",
        "def get_rand(n, l, r):\n",
        "  res = []\n",
        "  while len(res) < n:\n",
        "    cur = numpy.random.randint(l,r)\n",
        "    if cur in res:\n",
        "      continue\n",
        "    res.append(cur)\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CA7t11Utka2"
      },
      "outputs": [],
      "source": [
        "# the class mentioned above is loaded into the variable kexp with the matrix being tag_network\n",
        "\n",
        "kexp = kernelexponentiation(tag_network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx_4dq2ktskP"
      },
      "outputs": [],
      "source": [
        "K = kexp.get_kernel(0.00001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dv4H5Nw7CBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c3618e-d436-4dba-cfcb-10f398cc33d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-231ac3220a26>:25: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  distances_matrix[i][j] += K[x][y]*usertagweights[i][x]*usertagweights[j][y]\n",
            "<ipython-input-32-231ac3220a26>:26: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  distances_matrix[i][j] /= denoms[i]*denoms[j]\n"
          ]
        }
      ],
      "source": [
        "distances_matrix = np.zeros([num_users, num_users])\n",
        "\n",
        "# nonzerotags stores only the tags that are used by the user for each user\n",
        "nonzerotags = []\n",
        "denoms = []\n",
        "\n",
        "for i in range(num_users):\n",
        "  cur = []\n",
        "  denom = 0\n",
        "  for j in range(num_tags):\n",
        "    # considering if only weidht is greater than 0\n",
        "    if usertagweights[i][j] > 0:\n",
        "      # precalculating the denominators\n",
        "      denom += usertagweights[i][j]*usertagweights[i][j]\n",
        "      cur.append(j)\n",
        "  nonzerotags.append(cur)\n",
        "  denom **= 0.5\n",
        "  denoms.append(denom)\n",
        "\n",
        "for i in range(num_users):\n",
        "  for j in range(i):\n",
        "    for x in nonzerotags[i]:\n",
        "      for y in nonzerotags[j]:\n",
        "        # using kernel to find the similarity between the users\n",
        "        distances_matrix[i][j] += K[x][y]*usertagweights[i][x]*usertagweights[j][y]\n",
        "    distances_matrix[i][j] /= denoms[i]*denoms[j]\n",
        "    distances_matrix[j][i] = distances_matrix[i][j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_7etymy95lO"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# saving the value of beta calculated to this point into a csv file\n",
        "# the computational cost of calculating the kernel (K) is expensive and we cannot afford to compute the value of kernel everytime we run the notebook\n",
        "\n",
        "with open('drive/MyDrive/userdistances.csv', 'w', encoding='UTF8', newline='') as f:\n",
        "  f.truncate()\n",
        "  writer = csv.writer(f)\n",
        "  dummy = [x for x in range(num_users)]\n",
        "  writer.writerow(dummy)\n",
        "  writer.writerows(distances_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distances matrix saved, can run from here"
      ],
      "metadata": {
        "id": "lF9E5fCAghQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For loading the similarity matrix from the drive\n",
        "\n",
        "data = pd.read_csv(\"drive/MyDrive/userdistances.csv\")\n",
        "distances_matrix = data.to_numpy()\n",
        "\n",
        "num_users = len(distances_matrix)"
      ],
      "metadata": {
        "id": "p_lIcTTnhJZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agglomerative Hierarchical clustering"
      ],
      "metadata": {
        "id": "cbjxA7xy4M-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizes a 2d matrix from 0 to 1\n",
        "\n",
        "def normalize(mat):\n",
        "  mx = mat[0][0]\n",
        "  mn = mat[0][0]\n",
        "\n",
        "  for i in range(len(mat)):\n",
        "    for j in range(len(mat[i])):\n",
        "      mx = max(mat[i][j], mx)\n",
        "      mn = min(mat[i][j], mn)\n",
        "  \n",
        "  for i in range(len(mat)):\n",
        "    for j in range(len(mat[i])):\n",
        "      mat[i][j] = (mat[i][j]-mn)/(mx-mn)\n",
        "  \n",
        "  return mat"
      ],
      "metadata": {
        "id": "uNvq9-YQhBFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from matplotlib import pylab\n",
        "\n",
        "# For plotting the graphs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# takes the graph and saves the visualization into google drive\n",
        "\n",
        "def save_graph(graph,file_name, num_nodes, main_nodes = [], sizes = None):\n",
        "\n",
        "  # sizes of all the nodes is 200 if not specified\n",
        "  if sizes == None:\n",
        "    sizes = []\n",
        "    for x in range(num_nodes):\n",
        "      sizes.append(200)\n",
        "  plt.figure(num=None, figsize=(60, 60), dpi=40)\n",
        "  plt.axis('off')\n",
        "  fig = plt.figure(1)\n",
        "\n",
        "  # this layout avoids overlapping of clusters\n",
        "  pos = graphviz_layout(graph, prog = 'neato')\n",
        "  nx.draw_networkx_nodes(graph,pos, node_size = sizes)\n",
        "  nx.draw_networkx_edges(graph,pos, width = 1)\n",
        "  nx.draw_networkx_labels(graph,pos)\n",
        "\n",
        "  cut = 1.00\n",
        "  xmax = cut * max(xx for xx, yy in pos.values())\n",
        "  ymax = cut * max(yy for xx, yy in pos.values())\n",
        "  plt.xlim(0, xmax + 20)\n",
        "  plt.ylim(0, ymax + 20)\n",
        "\n",
        "  plt.savefig(file_name,bbox_inches=\"tight\")\n",
        "  pylab.close()\n",
        "  del fig"
      ],
      "metadata": {
        "id": "i8VftdJbg7a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heap datastructure has a major in the agglomerative clustering algorithm that has been used \n",
        "import heapq\n",
        "\n",
        "# For copying arrays and to avoid copying the addresses instead\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "# For clustering visualization\n",
        "import networkx as nx\n",
        "\n",
        "import pydot\n",
        "from networkx.drawing.nx_pydot import graphviz_layout\n",
        "\n",
        "\n",
        "# this class peforms the agglomerative clustering algorithm\n",
        "\n",
        "class agglomerative_clustering:\n",
        "\n",
        "  # setting all the appropriate values\n",
        "\n",
        "  def __init__(self, distance_matrix):\n",
        "    # self.dist stores the distance_matrices    \n",
        "    self.dist = distance_matrix\n",
        "    self.num_nodes = len(self.dist)\n",
        "    # performing normalization\n",
        "    self.dist = normalize(self.dist)\n",
        "    # self.num_clusters indicates the current number of clusters\n",
        "    self.num_clusters = self.num_nodes\n",
        "    # self.inter_cluster_distances is a 2d matrix that stores the summation of the distances among the nodes in the two clusters\n",
        "    self.inter_cluster_distances = deepcopy(self.dist)\n",
        "    # self.is_cluster_head indicates whether a node is head of its cluster\n",
        "    self.is_cluster_head = [True for x in range(self.num_nodes)]\n",
        "    # self.belongs_to stores the head node of the cluster which the current node belongs to\n",
        "    self.belongs_to = [x for x in range(self.num_nodes)]\n",
        "    # self.cluster[head_node] stores the nodes in the clusters whose head node is head_node \n",
        "    self.cluster = [[x] for x in range(self.num_nodes)]\n",
        "    # self.merges records all the merges that are done\n",
        "    self.merges = []\n",
        "    # self.heap stores the inter_cluster_distances and dynamically returns the closest clusters each time\n",
        "    # each element of the heap is in format [distances, head node of cluster 1, head node of cluster 2, iteration when pushed]\n",
        "    # note: the iteration of pushing is noted to identify whether to identify the distances after the push has happened\n",
        "    self.heap = []\n",
        "    # self.last_updated stores the last iteration in which the inter_cluster_distances between the two clusters was updated\n",
        "    self.last_updated = np.zeros([self.num_nodes, self.num_nodes])\n",
        "    # self.iter stores the current number of iterations performed\n",
        "    self.iter = 0\n",
        "\n",
        "    \n",
        "  def visualize(self):\n",
        "    # color values on the scale from 0 to 1 on 'Set2' color map\n",
        "    sizes = []\n",
        "    G=nx.Graph()\n",
        "    for i in range(self.num_nodes):\n",
        "      G.add_node(i)\n",
        "      # head nodes to be bigger in sizes to be identifiable\n",
        "      if self.is_cluster_head[i] and len(self.cluster[i]) > 4:\n",
        "        sizes.append(1000)\n",
        "      else:\n",
        "        sizes.append(300)\n",
        "\n",
        "    # adding edges to the graph\n",
        "    for edge in self.merges:\n",
        "      G.add_edge(edge[0],edge[1])\n",
        "\n",
        "    # specifying path for graph that needs to be saved\"\n",
        "    path = \"drive/MyDrive/UsersClusteringVisualizationGraph-\" + str(self.num_clusters) + \".pdf\"\n",
        "\n",
        "    save_graph(G,path, self.num_nodes, sizes = sizes)\n",
        "\n",
        "  # for getting the average distance between nodes in two clusters from inter_cluster_distances by dividing with appropriate value\n",
        "  def get_inter_cluster_distance(self, u, v):\n",
        "    return self.inter_cluster_distances[u][v]/(len(self.cluster[u])*len(self.cluster[v]))\n",
        "\n",
        "  # this is the point where clustering starts\n",
        "  def go(self, visualization = False, max_cluster_size = 10**10):\n",
        "    \n",
        "    # initially populating the heap\n",
        "\n",
        "    for i in range(self.num_nodes):\n",
        "      for j in range(self.num_nodes):\n",
        "        if i >= j:\n",
        "          continue\n",
        "        self.heap.append([-self.dist[i][j], i, j, self.iter])\n",
        "    # getting the top element heap (and finding the most similar clusters)\n",
        "    heapq.heapify(self.heap)\n",
        "    # performing the clustering untill all the nodes are merged\n",
        "    while len(self.heap) != 0:\n",
        "      # u represents to top element in the heap\n",
        "      u = heapq.heappop(self.heap)\n",
        "      if self.is_cluster_head[u[1]] and self.is_cluster_head[u[2]] and self.last_updated[u[1]][u[2]] == u[3] and (len(self.cluster[u[1]]) + len(self.cluster[u[2]]) <= max_cluster_size):\n",
        "        # incrementing the number of iterations\n",
        "        self.iter += 1\n",
        "        # combining the two clusters using the funtion that is defined below\n",
        "        self.combine(u[1],u[2])\n",
        "        self.num_clusters-=1\n",
        "        # printing the scores\n",
        "        if self.num_clusters%500 == 0 or (self.num_clusters <= 500 and self.num_clusters%100 == 0) or (self.num_clusters <= 100 and self.num_clusters%10 == 0) or (self.num_clusters <= 10):\n",
        "          if(visualization):\n",
        "            self.visualize()\n",
        "\n",
        "  # this function takes head nodes and combines the clusters corresponding to the head nodes\n",
        "  def combine(self, x, y):\n",
        "    # recording the merge\n",
        "    self.merges.append([x,y])\n",
        "    # new head of the cluster\n",
        "    # note: new head is always the head node of the largest cluster as the cluster with smaller size is merged into the cluster with larger size\n",
        "    new_head = x\n",
        "    # pilla_cluster_head represents the head of the smaller cluster\n",
        "    pilla_cluster_head = y\n",
        "    if len(self.cluster[x]) < len(self.cluster[y]):\n",
        "      new_head = y\n",
        "      pilla_cluster_head = x\n",
        "    \n",
        "    # changing the status of is_cluster_head for the head of smaller cluster as it is no longer head of the cluster\n",
        "    self.is_cluster_head[pilla_cluster_head] = False\n",
        "\n",
        "    # recalculatint the inter_cluster_distances for all clusters to the current cluster as new nodes are added into the cluster\n",
        "    for i in range(self.num_nodes):\n",
        "      if self.is_cluster_head[i] == False or (i == new_head):\n",
        "        continue\n",
        "      \n",
        "      self.inter_cluster_distances[i][new_head] += self.inter_cluster_distances[i][pilla_cluster_head]\n",
        "      self.inter_cluster_distances[new_head][i] = self.inter_cluster_distances[i][new_head]\n",
        "      # recording the latest updation\n",
        "      self.last_updated[i][new_head] = self.iter\n",
        "      self.last_updated[new_head][i] = self.iter\n",
        "      denom = (len(self.cluster[i])*(len(self.cluster[new_head]) + len(self.cluster[pilla_cluster_head])))\n",
        "      # pushing the updated value\n",
        "      heapq.heappush(self.heap, [-self.inter_cluster_distances[i][new_head]/denom, i, new_head, self.iter])\n",
        "\n",
        "    # merging the nodes into the new cluster\n",
        "    for node in self.cluster[pilla_cluster_head]:\n",
        "      self.belongs_to[node] = new_head\n",
        "      self.cluster[new_head].append(node)\n"
      ],
      "metadata": {
        "id": "1sliczxFgn5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performing clustering using the class declared above\n",
        "clustering_object = agglomerative_clustering(distances_matrix)\n",
        "# note: clusters are visualized if only specified\n",
        "clustering_object.go(visualization=True, max_cluster_size = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ4BDCa7jTPK",
        "outputId": "13709d93-c77f-4a98-8019-99cbf1c4190e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n",
            "<ipython-input-36-25f4483a08d9>:20: DeprecationWarning: nx.nx_pydot.graphviz_layout depends on the pydot package, which hasknown issues and is not actively maintained. Consider usingnx.nx_agraph.graphviz_layout instead.\n",
            "\n",
            "See https://github.com/networkx/networkx/issues/5723\n",
            "  pos = graphviz_layout(graph, prog = 'neato')\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}